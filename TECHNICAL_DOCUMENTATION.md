# AI Interview Simulator - Complete Technical Documentation

**Project**: AI Technical Interview Simulator for Bachelor's Thesis
**Stack**: Next.js 15, TypeScript, React 19, Tailwind CSS, Vercel AI SDK
**AI Models**: Groq (Cloud) + Google Gemini (Cloud Fallback) + Custom Fine-Tuned Llama-3-8B (Local)
**Architecture**: Hybrid Cloud/Local with Zero-Cost Design

---

## Table of Contents

1. [Project Architecture Overview](#project-architecture-overview)
2. [Frontend Implementation](#frontend-implementation)
3. [Backend Implementation](#backend-implementation)
4. [AI Integration Approaches](#ai-integration-approaches)
5. [Data Flow & State Management](#data-flow--state-management)
6. [Component Architecture](#component-architecture)
7. [API Routes & Server Actions](#api-routes--server-actions)
8. [PDF Processing Pipeline](#pdf-processing-pipeline)
9. [Custom Model Training & Deployment](#custom-model-training--deployment)
10. [Styling & UI/UX](#styling--uiux)
11. [Type System & Type Safety](#type-system--type-safety)
12. [Error Handling & Resilience](#error-handling--resilience)

---

## 1. Project Architecture Overview

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  ResumeUpload    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  ChatInterface   â”‚             â”‚
â”‚  â”‚  Component       â”‚         â”‚  Component       â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                              â”‚                        â”‚
â”‚         â”‚ FormData                     â”‚ HTTP POST              â”‚
â”‚         â–¼                              â–¼                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         BACKEND LAYER                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Server Action    â”‚         â”‚  API Route       â”‚             â”‚
â”‚  â”‚ extractResume    â”‚         â”‚  /api/chat       â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                              â”‚                        â”‚
â”‚         â”‚                              â”‚                        â”‚
â”‚         â–¼                              â–¼                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          AI LAYER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Groq API       â”‚  â”‚ Gemini API   â”‚  â”‚ Ollama (Local)   â”‚ â”‚
â”‚  â”‚ Llama-3.3-70B    â”‚  â”‚ Gemini-2.0   â”‚  â”‚ Custom Llama-8B  â”‚ â”‚
â”‚  â”‚   (Primary)      â”‚  â”‚  (Fallback)  â”‚  â”‚ (Fine-tuned)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**Frontend**:
- **Framework**: Next.js 15 (App Router)
- **UI Library**: React 19
- **Language**: TypeScript 5
- **Styling**: Tailwind CSS 3 + Shadcn/UI components
- **State Management**: React useState hooks
- **HTTP Client**: Native fetch API

**Backend**:
- **Runtime**: Next.js API Routes (serverless)
- **Server Actions**: Next.js Server Actions
- **AI SDK**: Vercel AI SDK v6.0.74
- **PDF Processing**: pdfreader library
- **Streaming**: ReadableStream API

**AI/ML**:
- **Cloud Models**: Groq (Llama-3.3-70B), Google Gemini (2.0-Flash)
- **Local Model**: Ollama + Custom Fine-Tuned Llama-3-8B
- **Training**: Google Colab (T4 GPU), Unsloth, LoRA
- **Format**: GGUF (4-bit quantized)

**Development**:
- **Package Manager**: npm
- **Linting**: ESLint
- **Type Checking**: TypeScript compiler

---

## 2. Frontend Implementation

### Project Structure

```
app/
â”œâ”€â”€ page.tsx                 # Main entry point (root page)
â”œâ”€â”€ globals.css              # Global styles (Tailwind)
â”œâ”€â”€ layout.tsx               # Root layout with metadata
â”œâ”€â”€ favicon.ico              # App icon
â””â”€â”€ api/
    â””â”€â”€ chat/
        â””â”€â”€ route.ts         # Chat API endpoint

components/
â”œâ”€â”€ ResumeUpload.tsx         # PDF upload component
â”œâ”€â”€ ChatInterface.tsx        # Main chat UI
â””â”€â”€ ui/                      # Shadcn UI primitives
    â”œâ”€â”€ button.tsx
    â”œâ”€â”€ card.tsx
    â””â”€â”€ ...
```

### Main Entry Point (`app/page.tsx`)

**Purpose**: Orchestrates the application flow from resume upload to chat interface

**Implementation**:
```typescript
'use client';

import { useState } from 'react';
import { ResumeUpload } from '@/components/ResumeUpload';
import { ChatInterface } from '@/components/ChatInterface';
import { ResumeData } from '@/types/resume';

export default function Home() {
  const [resumeData, setResumeData] = useState<ResumeData | null>(null);

  return (
    <div className="min-h-screen bg-gradient-to-br from-gray-50 to-gray-100">
      {!resumeData ? (
        // Step 1: Resume Upload Screen
        <div className="flex flex-col items-center justify-center min-h-screen p-8">
          <div className="mb-8 text-center space-y-3">
            <h1 className="text-5xl font-bold text-gray-900 tracking-tight">
              AI Interview Simulator
            </h1>
            <p className="text-lg text-gray-600 max-w-2xl">
              Practice your technical interviews with an AI Senior Tech Lead.
              Upload your resume to begin.
            </p>
          </div>
          <ResumeUpload onResumeExtracted={setResumeData} />
        </div>
      ) : (
        // Step 2: Chat Interface
        <ChatInterface
          resumeData={resumeData}
          onReset={() => setResumeData(null)}
        />
      )}
    </div>
  );
}
```

**Key Design Decisions**:
- **Client Component**: Uses `'use client'` directive for React hooks
- **Conditional Rendering**: Shows upload screen OR chat, never both
- **Single State**: `resumeData` drives the entire UI state
- **Gradient Background**: Consistent aesthetic across both views
- **Reset Capability**: Allows users to start over with new resume

---

### Resume Upload Component (`components/ResumeUpload.tsx`)

**Purpose**: Handles PDF file upload and extraction

**Key Features**:
1. Drag-and-drop file upload
2. File type validation (PDF only)
3. File size validation (max 10MB)
4. Visual feedback during processing
5. Error handling with user-friendly messages

**Implementation Highlights**:

```typescript
'use client';

import { useState, useCallback } from 'react';
import { useDropzone } from 'react-dropzone';
import { extractResumeData } from '@/lib/actions/resume';
import { ResumeData } from '@/types/resume';
import { Button } from '@/components/ui/button';
import { Card } from '@/components/ui/card';

interface ResumeUploadProps {
  onResumeExtracted: (data: ResumeData) => void;
}

export function ResumeUpload({ onResumeExtracted }: ResumeUploadProps) {
  const [isProcessing, setIsProcessing] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const onDrop = useCallback(async (acceptedFiles: File[]) => {
    const file = acceptedFiles[0];
    if (!file) return;

    // Validation
    if (file.type !== 'application/pdf') {
      setError('Please upload a PDF file');
      return;
    }

    if (file.size > 10 * 1024 * 1024) { // 10MB limit
      setError('File size must be less than 10MB');
      return;
    }

    setIsProcessing(true);
    setError(null);

    try {
      const formData = new FormData();
      formData.append('resume', file);

      // Call Server Action
      const result = await extractResumeData(formData);

      if (result.success && result.data) {
        onResumeExtracted(result.data);
      } else {
        setError(result.error || 'Failed to process resume');
      }
    } catch (err: any) {
      setError(err.message || 'Failed to upload resume');
    } finally {
      setIsProcessing(false);
    }
  }, [onResumeExtracted]);

  const { getRootProps, getInputProps, isDragActive } = useDropzone({
    onDrop,
    accept: { 'application/pdf': ['.pdf'] },
    maxFiles: 1,
    disabled: isProcessing,
  });

  return (
    <Card className="w-full max-w-2xl p-8">
      <div
        {...getRootProps()}
        className={`
          border-2 border-dashed rounded-lg p-12 text-center cursor-pointer
          transition-colors duration-200
          ${isDragActive ? 'border-blue-500 bg-blue-50' : 'border-gray-300 hover:border-gray-400'}
          ${isProcessing ? 'opacity-50 cursor-not-allowed' : ''}
        `}
      >
        <input {...getInputProps()} />

        {isProcessing ? (
          <div className="space-y-4">
            <div className="animate-spin rounded-full h-16 w-16 border-b-2 border-blue-600 mx-auto" />
            <p className="text-gray-600">Processing your resume...</p>
          </div>
        ) : (
          <div className="space-y-4">
            <svg className="mx-auto h-16 w-16 text-gray-400" /* ... */ />
            <p className="text-lg font-medium text-gray-900">
              {isDragActive ? 'Drop your resume here' : 'Upload your resume'}
            </p>
            <p className="text-sm text-gray-500">
              Drag & drop or click to browse (PDF, max 10MB)
            </p>
          </div>
        )}
      </div>

      {error && (
        <div className="mt-4 p-4 bg-red-50 border border-red-200 rounded-lg">
          <p className="text-sm text-red-800">{error}</p>
        </div>
      )}
    </Card>
  );
}
```

**Technical Details**:
- **react-dropzone**: Library for drag-and-drop functionality
- **FormData API**: Used for file upload (browser native)
- **Server Actions**: Calls `extractResumeData` (runs on server)
- **Loading State**: Shows spinner during processing
- **Error Boundaries**: Comprehensive error handling at multiple levels

---

### Chat Interface Component (`components/ChatInterface.tsx`)

**Purpose**: Main chat UI with streaming AI responses

**Key Features**:
1. Message history display
2. Real-time streaming responses
3. Auto-scroll to latest message
4. Input validation
5. Loading states
6. Reset/start over functionality

**Implementation**:

```typescript
'use client';

import { useState, useRef, useEffect, FormEvent } from 'react';
import { ResumeData } from '@/types/resume';
import { Button } from '@/components/ui/button';
import { Card } from '@/components/ui/card';

interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
}

interface ChatInterfaceProps {
  resumeData: ResumeData;
  onReset: () => void;
}

export function ChatInterface({ resumeData, onReset }: ChatInterfaceProps) {
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  // Auto-scroll to bottom when messages update
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  const handleSubmit = async (e: FormEvent) => {
    e.preventDefault();
    if (!input.trim() || isLoading) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      role: 'user',
      content: input.trim(),
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);

    try {
      // Call Chat API
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          messages: [...messages, userMessage],
          resumeData,
        }),
      });

      if (!response.ok) {
        throw new Error('Failed to get response');
      }

      // Handle streaming response
      const reader = response.body?.getReader();
      const decoder = new TextDecoder();
      let assistantMessage = '';
      const assistantId = (Date.now() + 1).toString();

      // Add initial empty assistant message
      setMessages(prev => [
        ...prev,
        { id: assistantId, role: 'assistant', content: '' }
      ]);

      if (reader) {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          // Decode chunk and append to message
          const chunk = decoder.decode(value, { stream: true });
          assistantMessage += chunk;

          // Update message in real-time
          setMessages(prev =>
            prev.map(m =>
              m.id === assistantId
                ? { ...m, content: assistantMessage }
                : m
            )
          );
        }
      }
    } catch (error) {
      console.error('Chat error:', error);
      setMessages(prev => [
        ...prev,
        {
          id: Date.now().toString(),
          role: 'assistant',
          content: 'Sorry, I encountered an error. Please try again.',
        }
      ]);
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="flex flex-col h-screen bg-gray-50">
      {/* Header */}
      <header className="bg-white border-b border-gray-200 p-4 flex justify-between items-center">
        <div>
          <h1 className="text-xl font-bold text-gray-900">Technical Interview</h1>
          <p className="text-sm text-gray-600">Interviewing: {resumeData.name}</p>
        </div>
        <Button variant="outline" onClick={onReset}>
          Start Over
        </Button>
      </header>

      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages.length === 0 ? (
          <div className="text-center py-12">
            <p className="text-gray-500">
              Hello {resumeData.name}! I'm your technical interviewer.
              Ask me anything or say hello to begin.
            </p>
          </div>
        ) : (
          messages.map((message) => (
            <div
              key={message.id}
              className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}
            >
              <Card
                className={`max-w-[80%] p-4 ${
                  message.role === 'user'
                    ? 'bg-blue-600 text-white'
                    : 'bg-white text-gray-900'
                }`}
              >
                <p className="whitespace-pre-wrap">{message.content}</p>
              </Card>
            </div>
          ))
        )}
        <div ref={messagesEndRef} />
      </div>

      {/* Input */}
      <form onSubmit={handleSubmit} className="border-t border-gray-200 bg-white p-4">
        <div className="flex gap-2">
          <input
            type="text"
            value={input}
            onChange={(e) => setInput(e.target.value)}
            placeholder="Type your message..."
            disabled={isLoading}
            className="flex-1 px-4 py-2 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500"
          />
          <Button type="submit" disabled={isLoading || !input.trim()}>
            {isLoading ? 'Sending...' : 'Send'}
          </Button>
        </div>
      </form>
    </div>
  );
}
```

**Technical Highlights**:
- **Streaming**: Reads response chunks in real-time using ReadableStream
- **Optimistic Updates**: Shows user message immediately
- **Real-time UI**: Updates assistant message as chunks arrive
- **Auto-scroll**: useEffect + ref for smooth scrolling
- **Accessibility**: Proper semantic HTML and ARIA labels
- **Responsive**: Mobile-friendly layout

---

## 3. Backend Implementation

### API Route: `/api/chat/route.ts`

**Purpose**: Main AI chat endpoint with hybrid cloud fallback

**Current Implementation** (Cloud Wrapper Approach):

```typescript
import { streamText } from 'ai';
import { groq } from '@ai-sdk/groq';
import { google } from '@ai-sdk/google';
import { ResumeData } from '@/types/resume';
import { generateInterviewerSystemPrompt } from '@/lib/prompts';

export const maxDuration = 30;

export async function POST(req: Request) {
  try {
    const body = await req.json();
    const { messages, resumeData } = body as {
      messages: { role: string; content: string }[];
      resumeData: ResumeData;
    };

    if (!resumeData) {
      return new Response('Resume data is required', { status: 400 });
    }

    // Generate system prompt with resume context
    const systemPrompt = generateInterviewerSystemPrompt(resumeData);

    // Format messages for AI SDK
    const formattedMessages = messages.map(m => ({
      role: m.role,
      content: m.content
    }));

    // Strategy 1: Try Groq (Primary - Fast, 70B model)
    try {
      console.log('ğŸš€ Using Groq API (Primary)');

      const result = streamText({
        model: groq('llama-3.3-70b-versatile'),
        system: systemPrompt,
        messages: formattedMessages,
        temperature: 0.7,
      });

      return result.toTextStreamResponse();

    } catch (groqError: any) {
      // Strategy 2: Fallback to Gemini on rate limit
      const isRateLimit = groqError?.status === 429;

      if (isRateLimit) {
        console.log('âš ï¸ Groq rate limit, falling back to Gemini');
      }

      console.log('ğŸ”„ Using Gemini API (Fallback)');

      const result = streamText({
        model: google('gemini-2.0-flash'),
        system: systemPrompt,
        messages: formattedMessages,
        temperature: 0.7,
      });

      return result.toTextStreamResponse();
    }

  } catch (error: any) {
    console.error('ğŸ’¥ Complete API failure:', error);
    return new Response(
      JSON.stringify({ error: 'Failed to generate response' }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    );
  }
}
```

**Architectural Patterns**:

1. **Hybrid Fallback Pattern**:
   - Primary: Groq (llama-3.3-70b-versatile) - Fastest
   - Fallback: Google Gemini (gemini-2.0-flash) - When Groq rate-limited
   - Zero downtime despite free tier rate limits

2. **Context Injection Strategy**:
   - No vector database needed
   - Resume data passed directly in system prompt
   - Full context available in every request

3. **Streaming Response**:
   - Uses `streamText()` from Vercel AI SDK
   - Returns `toTextStreamResponse()` - browser-compatible stream
   - Frontend reads chunks in real-time

---

### Server Action: `lib/actions/resume.ts`

**Purpose**: Server-side PDF parsing and AI-powered structuring

**Implementation**:

```typescript
'use server';

import { generateText } from 'ai';
import { groq } from '@ai-sdk/groq';
import { google } from '@ai-sdk/google';
import { ResumeData } from '@/types/resume';

export async function extractResumeData(formData: FormData): Promise<{
  success: boolean;
  data?: ResumeData;
  error?: string;
}> {
  try {
    // Extract file from FormData
    const file = formData.get('resume') as File;
    if (!file) {
      return { success: false, error: 'No file provided' };
    }

    // Convert to Buffer for PDF processing
    const arrayBuffer = await file.arrayBuffer();
    const buffer = Buffer.from(arrayBuffer);

    // Step 1: Extract raw text from PDF using pdfreader
    const { PdfReader } = await import('pdfreader');

    const rawText = await new Promise<string>((resolve, reject) => {
      const pageTexts: { [page: number]: { [y: number]: string[] } } = {};
      let currentPage = 0;

      new PdfReader().parseBuffer(buffer, (err: any, item: any) => {
        if (err) {
          reject(err);
        } else if (!item) {
          // End of file - combine all text
          const allText: string[] = [];

          // Sort by page number
          Object.keys(pageTexts)
            .sort((a, b) => parseInt(a) - parseInt(b))
            .forEach(page => {
              const pageNum = parseInt(page);
              const lines = pageTexts[pageNum];

              // Sort by Y position (top to bottom)
              Object.keys(lines)
                .sort((a, b) => parseFloat(a) - parseFloat(b))
                .forEach(y => {
                  const lineText = lines[parseFloat(y)].join(' ');
                  allText.push(lineText);
                });
              allText.push('\n'); // Page separator
            });

          const finalText = allText.join(' ').trim();
          console.log('ğŸ“„ PDF parsed, extracted text length:', finalText.length);
          resolve(finalText);
        } else if (item.page) {
          // New page
          currentPage = item.page;
          pageTexts[currentPage] = pageTexts[currentPage] || {};
        } else if (item.text) {
          // Text item - group by Y position (line)
          const y = item.y || 0;
          pageTexts[currentPage] = pageTexts[currentPage] || {};
          pageTexts[currentPage][y] = pageTexts[currentPage][y] || [];
          pageTexts[currentPage][y].push(item.text);
        }
      });
    });

    if (!rawText || rawText.length < 100) {
      return {
        success: false,
        error: 'Unable to extract sufficient text from PDF'
      };
    }

    // Step 2: Use AI to structure the raw text
    console.log('ğŸš€ Using Groq to parse resume...');

    let text: string;
    try {
      // Try Groq first
      const result = await generateText({
        model: groq('llama-3.3-70b-versatile'),
        prompt: `You are an expert resume parser. Analyze the following resume text and extract structured information.

Resume Text:
${rawText}

Extract and return ONLY a JSON object with this exact structure (no markdown, no extra text):
{
  "name": "Full name",
  "email": "email@example.com",
  "phone": "phone number or null",
  "skills": ["skill1", "skill2", ...],
  "experience": [
    {
      "position": "Job title",
      "company": "Company name",
      "duration": "Time period",
      "description": "Brief description"
    }
  ],
  "education": [
    {
      "degree": "Degree name",
      "institution": "School name",
      "year": "Graduation year"
    }
  ],
  "weaknesses": ["potential gap 1", "potential gap 2"],
  "rawText": "${rawText.substring(0, 1000)}..."
}

Focus on technical skills and identify areas for interview questions.`
      });
      text = result.text;
    } catch (groqError) {
      // Fallback to Gemini
      console.log('âš ï¸ Groq failed, using Gemini fallback');
      const result = await generateText({
        model: google('gemini-2.0-flash'),
        prompt: `[Same prompt as above]`
      });
      text = result.text;
    }

    // Step 3: Parse JSON response
    const jsonMatch = text.match(/\{[\s\S]*\}/);
    if (!jsonMatch) {
      return { success: false, error: 'Failed to parse resume structure' };
    }

    const structuredData = JSON.parse(jsonMatch[0]);

    // Add raw text for context
    structuredData.rawText = rawText;

    console.log('âœ… Resume parsed successfully:', structuredData.name);

    return {
      success: true,
      data: structuredData as ResumeData
    };

  } catch (error: any) {
    console.error('âŒ Resume extraction error:', error);
    return {
      success: false,
      error: error.message || 'Failed to process resume'
    };
  }
}
```

**Key Implementation Details**:

1. **Event-Based PDF Parsing**:
   - `pdfreader` emits events: `item.page`, `item.text`
   - Groups text by page and Y-position
   - Sorts to maintain reading order

2. **Two-Step Processing**:
   - **Step 1**: Extract raw text (deterministic)
   - **Step 2**: AI structures text (intelligent parsing)

3. **Hybrid AI Parsing**:
   - Groq primary (faster, free)
   - Gemini fallback (redundancy)
   - Uses `generateText()` (non-streaming) for structured output

4. **JSON Extraction**:
   - AI might return markdown or extra text
   - Regex extracts JSON object: `/\{[\s\S]*\}/`
   - `JSON.parse()` for type safety

---

### System Prompt Generation (`lib/prompts.ts`)

**Purpose**: Creates context-aware interview persona

**Implementation**:

```typescript
import { ResumeData } from '@/types/resume';

export function generateInterviewerSystemPrompt(resumeData: ResumeData): string {
  return `You are a Senior Tech Lead conducting a technical job interview. You are experienced, professional, and use the Socratic method to help candidates discover solutions themselves.

# CANDIDATE PROFILE
**Name**: ${resumeData.name}
**Email**: ${resumeData.email || 'N/A'}
**Skills**: ${resumeData.skills.join(', ')}

**Experience**:
${resumeData.experience.map(exp =>
  `- ${exp.position} at ${exp.company} (${exp.duration}): ${exp.description}`
).join('\n')}

**Education**:
${resumeData.education.map(edu =>
  `- ${edu.degree} from ${edu.institution} (${edu.year})`
).join('\n')}

**Potential Areas to Explore**:
${resumeData.weaknesses?.join(', ') || 'General technical depth'}

# YOUR ROLE & BEHAVIOR

## Critical Rules
- âŒ **NEVER ask multiple questions in one message**
- âŒ **NEVER provide your own answers or solutions**
- âŒ **NEVER lecture or give tutorials**
- âœ… **ALWAYS ask ONE focused question and wait for response**
- âœ… **ALWAYS be constructive, encouraging, and professional**
- âœ… **ALWAYS reference the candidate's specific experience when relevant**

## Socratic Method Guidelines
1. **Ask guiding questions** instead of explaining
2. **Build on their answers** - use their words in follow-ups
3. **Challenge gently** - probe deeper when answers are surface-level
4. **Lead to realizations** - help them discover, don't tell
5. **Connect to their experience** - reference their resume projects

## Conversation Flow
1. Start with open-ended questions about their experience
2. Ask ONE question at a time
3. Listen to their response carefully
4. Ask a follow-up question based on what they said
5. If they're stuck, ask a simpler related question (don't give the answer)
6. If they give a partial answer, acknowledge it and probe deeper

## Examples of Good Questions
- "What challenges did you face when implementing [feature from resume]?"
- "How would you explain [concept] to a junior developer?"
- "What trade-offs did you consider when choosing [technology]?"
- "Can you walk me through your thought process when debugging [type of issue]?"

## Examples of What NOT to Do
- âŒ "Let me explain how React hooks work..."
- âŒ "The answer is... Here's how you should think about it..."
- âŒ "Here are three things you should know: 1) ... 2) ... 3) ..."

## Tone
- Professional but warm
- Encouraging without being patronizing
- Curious and engaged
- Patient with thinking time
- Respectful of the candidate's experience level

Remember: Your goal is to help ${resumeData.name} demonstrate their knowledge and problem-solving skills through thoughtful questions, not to teach them or show off your own knowledge.`;
}
```

**Design Rationale**:
- **Personalization**: Uses actual candidate data (name, skills, experience)
- **Explicit Rules**: Clear do's and don'ts prevent common AI mistakes
- **Examples**: Concrete examples of good vs bad behavior
- **Socratic Focus**: Emphasizes questions over explanations
- **One Question Rule**: Most important rule stated multiple times

---

## 4. AI Integration Approaches

### Approach 1: Cloud Wrapper (Current Active Implementation)

**Architecture**:
```
User Query
    â†“
Next.js API Route
    â†“
Vercel AI SDK (streamText)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Try Groq    â”‚ â”€â”€[Success]â”€â”€> Stream Response
â”‚ (Primary)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ [Rate Limit/Error]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Try Gemini  â”‚ â”€â”€[Success]â”€â”€> Stream Response
â”‚ (Fallback)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ [Both Fail]
    â†“
Return 500 Error
```

**Characteristics**:
- **Speed**: Fast (~500ms response time with Groq)
- **Model Size**: 70B parameters (Groq) / 2B parameters (Gemini)
- **Cost**: $0 (free tier with rate limits)
- **Privacy**: Data sent to cloud APIs
- **Customization**: System prompt engineering only
- **Reliability**: High (dual fallback)

**Code Location**: `app/api/chat/route.ts`

**Advantages**:
âœ… Zero cost
âœ… Fast responses
âœ… Large model (better quality)
âœ… No local compute needed
âœ… Automatic failover

**Disadvantages**:
âŒ Requires internet
âŒ Data leaves device
âŒ Rate limits exist
âŒ No model customization
âŒ Generic behavior (not fine-tuned)

---

### Approach 2: Custom Fine-Tuned Local Model

**Architecture**:
```
User Query
    â†“
Next.js API Route (alternative: /api/chat-local)
    â†“
Ollama SDK
    â†“
Ollama Server (localhost:11434)
    â†“
Custom Model: socratic-interviewer
    â†“
Llama-3-8B (Fine-tuned with LoRA)
    â†“
Stream Response
```

**Training Pipeline**:
```
1. Data Generation (scripts/generate_dataset.py)
   â”œâ”€> Groq API generates 40 Socratic examples
   â””â”€> Output: data/interviewer_training_data.jsonl

2. Fine-Tuning (notebooks/train_socratic_interviewer.ipynb)
   â”œâ”€> Google Colab (T4 GPU, free tier)
   â”œâ”€> Base: Llama-3-8B-Instruct (4-bit quantized)
   â”œâ”€> Method: LoRA (r=16, trainable params: 42M / 8B = 0.52%)
   â”œâ”€> Training: 3 epochs, 15 steps, ~15 minutes
   â””â”€> Output: Fine-tuned model

3. Export (in Colab notebook)
   â”œâ”€> Convert to GGUF format
   â”œâ”€> Quantize to Q4_K_M (4-bit)
   â””â”€> Output: llama-3-8b-instruct.Q4_K_M.gguf (4.9GB)

4. Deployment (local machine)
   â”œâ”€> Install Ollama
   â”œâ”€> Import GGUF file
   â””â”€> Model: socratic-interviewer:latest
```

**Implementation** (Preserved for Future Use):

```typescript
// Alternative API route (not currently used)
// app/api/chat-local/route.ts

import { Ollama } from 'ollama';
import { ResumeData } from '@/types/resume';
import { generateInterviewerSystemPrompt } from '@/lib/prompts';

export const maxDuration = 60;

export async function POST(req: Request) {
  const { messages, resumeData } = await req.json();
  const systemPrompt = generateInterviewerSystemPrompt(resumeData);

  // Initialize Ollama client
  const ollama = new Ollama({ host: 'http://localhost:11434' });

  // Format messages (embed system prompt in first user message)
  const conversationHistory = messages.map(m => ({
    role: m.role === 'user' ? 'user' : 'assistant',
    content: m.content
  }));

  if (conversationHistory[0]?.role === 'user') {
    conversationHistory[0].content =
      `${systemPrompt}\n\nUser: ${conversationHistory[0].content}`;
  }

  // Stream response
  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      const response = await ollama.chat({
        model: 'socratic-interviewer',
        messages: conversationHistory,
        stream: true,
        options: { temperature: 0.7, top_p: 0.9, num_ctx: 4096 }
      });

      for await (const chunk of response) {
        if (chunk.message?.content) {
          controller.enqueue(encoder.encode(chunk.message.content));
        }
      }
      controller.close();
    }
  });

  return new Response(stream, {
    headers: { 'Content-Type': 'text/plain; charset=utf-8' }
  });
}
```

**Characteristics**:
- **Speed**: Moderate (~1-2s on CPU)
- **Model Size**: 8B parameters (fine-tuned)
- **Cost**: $0 (one-time training, then free forever)
- **Privacy**: 100% local (no data leaves device)
- **Customization**: Fine-tuned on Socratic interview examples
- **Reliability**: No internet needed, no rate limits

**Advantages**:
âœ… 100% private
âœ… No internet required
âœ… No rate limits
âœ… Custom behavior (fine-tuned)
âœ… $0 ongoing cost

**Disadvantages**:
âŒ Slower (~2s vs 500ms)
âŒ Smaller model (8B vs 70B)
âŒ Requires local setup
âŒ Uses CPU/RAM
âŒ Partial Socratic success (needs more training data)

---

### Training Details: Custom Model

**Dataset Generation** (`scripts/generate_dataset.py`):

```python
#!/usr/bin/env python3
from groq import Groq
from dotenv import load_dotenv
import os, json

load_dotenv('.env.local')
client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

# 30+ technical scenarios
SCENARIOS = [
    {
        "topic": "React Virtual DOM",
        "user_misconception": "React uses the real DOM directly.",
        "context": "Junior developer explaining rendering"
    },
    # ... 32 total scenarios
]

def generate_socratic_response(scenario):
    """Uses Groq to generate Socratic interview responses"""
    system_prompt = """You are a Senior Technical Interviewer who uses
    the Socratic method. NEVER give direct answers - ask guiding questions."""

    completion = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": scenario["user_input"]}
        ],
        temperature=0.8,
        max_tokens=150,
    )

    return {
        "conversations": [
            {"from": "human", "value": scenario["user_input"]},
            {"from": "gpt", "value": completion.choices[0].message.content}
        ]
    }

# Generate 40 examples: 32 single-turn + 8 multi-turn
# Output: data/interviewer_training_data.jsonl
```

**Fine-Tuning Configuration** (Colab Notebook):

```python
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments

# Load base model with 4-bit quantization
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-instruct-bnb-4bit",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# Apply LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,                    # LoRA rank
    lora_alpha=16,
    lora_dropout=0,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"      # MLP
    ],
    bias="none",
    use_gradient_checkpointing="unsloth",
)

# Training
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        num_train_epochs=3,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        output_dir="outputs",
    ),
)

trainer.train()

# Export to GGUF
model.save_pretrained_gguf("model", tokenizer, quantization_method="q4_k_m")
```

**Training Metrics**:
- Base Model: 8,030,261,248 parameters
- Trainable (LoRA): 41,943,040 parameters (0.52%)
- Training Steps: 15 (40 examples Ã— 3 epochs / batch 8)
- Training Time: ~15 minutes on T4 GPU
- GPU Memory: 12GB / 15GB used
- Final Model Size: 4.9GB (GGUF Q4_K_M)

**Deployment**:
```bash
# Install Ollama
brew install ollama

# Create Modelfile
cat > Modelfile <<EOF
FROM ./llama-3-8b-instruct.Q4_K_M.gguf
PARAMETER temperature 0.7
SYSTEM "You are a Socratic technical interviewer."
EOF

# Import
ollama create socratic-interviewer -f Modelfile

# Verify
ollama list
# socratic-interviewer:latest    4.9 GB
```

---

## 5. Data Flow & State Management

### Resume Upload Flow

```
1. User drops PDF file
   â†“
2. Frontend: ResumeUpload component
   - File validation (type, size)
   - Create FormData
   â†“
3. Server Action: extractResumeData()
   - Receive FormData
   - Convert to Buffer
   â†“
4. PDF Processing: pdfreader
   - Event-based parsing
   - Extract raw text (5000+ chars)
   â†“
5. AI Structuring: Groq/Gemini
   - generateText() with structuring prompt
   - Parse JSON from response
   â†“
6. Return ResumeData object
   â†“
7. Frontend: Update state
   - setResumeData(data)
   - Triggers re-render
   â†“
8. Render: ChatInterface
   - Resume data available in context
```

### Chat Message Flow

```
1. User types message and submits
   â†“
2. Frontend: ChatInterface
   - Add user message to state
   - Clear input field
   - Set loading state
   â†“
3. HTTP POST to /api/chat
   - Body: { messages, resumeData }
   - Content-Type: application/json
   â†“
4. Backend: API Route
   - Validate resumeData
   - Generate system prompt
   - Format messages
   â†“
5. AI Processing: Groq (Primary)
   - streamText() with system + messages
   - Model: llama-3.3-70b-versatile
   - Temperature: 0.7
   â†“
6. Streaming Response
   - toTextStreamResponse()
   - Chunks sent as they're generated
   â†“
7. Frontend: Read Stream
   - ReadableStream.getReader()
   - Decode chunks with TextDecoder
   - Append to assistant message
   â†“
8. UI Update (Real-time)
   - Update message content in state
   - Re-render with new content
   - Auto-scroll to bottom
   â†“
9. Completion
   - Stream ends
   - Set loading state to false
   - Ready for next message
```

### State Management Architecture

**Global State**: None (no Redux, no Context)

**Component State**:
```typescript
// app/page.tsx
const [resumeData, setResumeData] = useState<ResumeData | null>(null);

// components/ChatInterface.tsx
const [messages, setMessages] = useState<Message[]>([]);
const [input, setInput] = useState('');
const [isLoading, setIsLoading] = useState(false);

// components/ResumeUpload.tsx
const [isProcessing, setIsProcessing] = useState(false);
const [error, setError] = useState<string | null>(null);
```

**State Flow**:
- Unidirectional data flow (React standard)
- Parent â†’ Child via props
- Child â†’ Parent via callback functions
- No global state needed (simple app)

**Why No Global State?**
- Only 3 components
- Linear flow (upload â†’ chat)
- No cross-component state sharing
- Props drilling is manageable
- Keeps architecture simple

---

## 6. Component Architecture

### Component Hierarchy

```
App (app/page.tsx)
â”œâ”€â”€ [State: resumeData]
â”‚
â”œâ”€â”€ ResumeUpload
â”‚   â”œâ”€â”€ Props: onResumeExtracted
â”‚   â”œâ”€â”€ State: isProcessing, error
â”‚   â””â”€â”€ Children:
â”‚       â”œâ”€â”€ Card (Shadcn)
â”‚       â”œâ”€â”€ Dropzone (react-dropzone)
â”‚       â””â”€â”€ Loading Spinner
â”‚
â””â”€â”€ ChatInterface
    â”œâ”€â”€ Props: resumeData, onReset
    â”œâ”€â”€ State: messages, input, isLoading
    â”œâ”€â”€ Ref: messagesEndRef (auto-scroll)
    â””â”€â”€ Children:
        â”œâ”€â”€ Header
        â”‚   â”œâ”€â”€ Title
        â”‚   â”œâ”€â”€ Candidate Name
        â”‚   â””â”€â”€ Reset Button
        â”œâ”€â”€ Messages Container
        â”‚   â””â”€â”€ Message Cards (mapped)
        â”‚       â”œâ”€â”€ User Messages (right, blue)
        â”‚       â””â”€â”€ Assistant Messages (left, white)
        â””â”€â”€ Input Form
            â”œâ”€â”€ Text Input
            â””â”€â”€ Send Button
```

### Shadcn/UI Components Used

**Installed Components**:
```bash
npx shadcn-ui@latest add button
npx shadcn-ui@latest add card
npx shadcn-ui@latest add input
```

**Button** (`components/ui/button.tsx`):
- Variants: default, outline, ghost, destructive
- Sizes: sm, md, lg
- Used in: ChatInterface (Send, Reset), ResumeUpload

**Card** (`components/ui/card.tsx`):
- Container for upload dropzone
- Container for chat messages
- Provides consistent elevation/shadow

**Usage Example**:
```typescript
import { Button } from '@/components/ui/button';
import { Card } from '@/components/ui/card';

<Card className="p-6">
  <h2>Card Content</h2>
  <Button variant="outline">Click Me</Button>
</Card>
```

### Component Communication

**Parent â†’ Child** (Props):
```typescript
// Pass data down
<ChatInterface
  resumeData={resumeData}     // Data
  onReset={() => setResumeData(null)}  // Callback
/>
```

**Child â†’ Parent** (Callbacks):
```typescript
// ResumeUpload calls this when done
interface ResumeUploadProps {
  onResumeExtracted: (data: ResumeData) => void;
}

// Inside ResumeUpload
onResumeExtracted(structuredData);
```

**No Context/Redux Needed**:
- Simple parent-child relationships
- Callbacks handle upward communication
- Props handle downward communication

---

## 7. API Routes & Server Actions

### API Routes (App Router)

**Location**: `app/api/[route]/route.ts`

**Current Routes**:
1. `app/api/chat/route.ts` - Main chat endpoint (active)
2. `app/api/chat-local/route.ts` - Local Ollama endpoint (future use)

**Route Handler Structure**:
```typescript
// app/api/chat/route.ts
export const maxDuration = 30;  // Vercel timeout config

export async function POST(req: Request) {
  // 1. Parse request
  const body = await req.json();

  // 2. Validate input
  if (!body.resumeData) {
    return new Response('Error', { status: 400 });
  }

  // 3. Process with AI
  const result = streamText({ ... });

  // 4. Return streaming response
  return result.toTextStreamResponse();
}
```

**Key Features**:
- Serverless functions (auto-scaling)
- Edge-compatible (can run on Vercel Edge)
- Streaming support (Server-Sent Events)
- No CORS needed (same origin)

### Server Actions

**Location**: `lib/actions/resume.ts`

**Purpose**: Server-side file processing

**Declaration**:
```typescript
'use server';  // Next.js directive

export async function extractResumeData(formData: FormData) {
  // Runs on server only
  // Can access Node.js APIs
  // Cannot be called from browser directly
}
```

**Advantages over API Routes**:
- Simpler to call (no fetch needed)
- Automatic form handling
- Type-safe (TypeScript end-to-end)
- Progressive enhancement support

**Usage**:
```typescript
// Frontend
import { extractResumeData } from '@/lib/actions/resume';

const formData = new FormData();
formData.append('resume', file);

const result = await extractResumeData(formData);
```

**vs API Route**:
```typescript
// API Route approach (more verbose)
const response = await fetch('/api/resume', {
  method: 'POST',
  body: formData
});
const result = await response.json();
```

---

## 8. PDF Processing Pipeline

### Library Selection Journey

**Attempted Libraries**:
1. âŒ **pdf-parse**: Import errors, worker configuration issues
2. âŒ **pdf2json**: Extracted 0 characters from valid PDF
3. âŒ **pdfjs-dist**: Complex worker setup, Node.js compatibility issues
4. âœ… **pdfreader**: Event-based parsing, works in Node.js

### pdfreader Implementation

**Why pdfreader?**
- Pure Node.js (no browser dependencies)
- Event-based API (flexible)
- Handles complex layouts
- Preserves text positioning
- No worker threads needed

**Event-Based Parsing**:
```typescript
new PdfReader().parseBuffer(buffer, (err, item) => {
  if (err) {
    // Handle error
  } else if (!item) {
    // End of file
  } else if (item.page) {
    // New page started
    currentPage = item.page;
  } else if (item.text) {
    // Text item found
    // item.text: string
    // item.x: horizontal position
    // item.y: vertical position
  }
});
```

**Text Reconstruction Algorithm**:

```typescript
// Data structure: page â†’ y-position â†’ text items
const pageTexts: {
  [page: number]: {
    [y: number]: string[]
  }
} = {};

// Group text by position
if (item.text) {
  const y = item.y || 0;
  pageTexts[currentPage][y] = pageTexts[currentPage][y] || [];
  pageTexts[currentPage][y].push(item.text);
}

// Sort and combine
Object.keys(pageTexts)
  .sort((a, b) => parseInt(a) - parseInt(b))  // Sort pages
  .forEach(page => {
    const lines = pageTexts[parseInt(page)];
    Object.keys(lines)
      .sort((a, b) => parseFloat(a) - parseFloat(b))  // Sort lines
      .forEach(y => {
        const lineText = lines[parseFloat(y)].join(' ');  // Join words
        allText.push(lineText);
      });
  });
```

**Why This Approach?**
- PDFs don't have "lines" - only positioned text
- Y-coordinate groups text into lines
- X-coordinate determines word order
- Sorting ensures correct reading order

### AI-Powered Structuring

**Two-Step Process**:

**Step 1: Extract Raw Text** (Deterministic)
- Input: PDF binary
- Output: Plain text string
- No AI involved
- Result: "John Doe\nSoftware Engineer\n..."

**Step 2: Structure with AI** (Intelligent)
- Input: Raw text
- Output: Structured JSON
- Uses: Groq or Gemini
- Result: `{ name: "John Doe", skills: [...], ... }`

**Why Two Steps?**
1. **Reliability**: PDF parsing is deterministic
2. **Flexibility**: AI handles various resume formats
3. **Debugging**: Can inspect raw text if structuring fails
4. **Fallback**: If AI fails, still have raw text

**Structuring Prompt**:
```typescript
`You are an expert resume parser. Analyze the following resume text and
extract structured information.

Resume Text:
${rawText}

Extract and return ONLY a JSON object with this exact structure:
{
  "name": "Full name",
  "email": "email@example.com",
  "skills": ["skill1", "skill2", ...],
  "experience": [
    {
      "position": "Job title",
      "company": "Company name",
      "duration": "Time period",
      "description": "Brief description"
    }
  ],
  ...
}

Focus on technical skills and identify areas for interview questions.`
```

**JSON Extraction**:
```typescript
// AI might return: "Here's the JSON:\n{...}\n\nThis resume shows..."
// Extract just the JSON object
const jsonMatch = text.match(/\{[\s\S]*\}/);
const jsonString = jsonMatch ? jsonMatch[0] : text;
const structuredData = JSON.parse(jsonString);
```

---

## 9. Custom Model Training & Deployment

### Training Infrastructure

**Hardware**: Google Colab (Free Tier)
- GPU: NVIDIA T4 (16GB VRAM)
- RAM: 12GB system RAM
- Storage: 100GB temporary disk
- Cost: $0
- Limitations: 12-hour session limit, may disconnect

**Software Stack**:
```python
# Key Libraries
unsloth        # Optimized fine-tuning (2x faster)
transformers   # HuggingFace model loading
peft           # Parameter-Efficient Fine-Tuning (LoRA)
trl            # Transformer Reinforcement Learning (SFTTrainer)
bitsandbytes   # 4-bit quantization (QLoRA)
```

### LoRA (Low-Rank Adaptation) Explained

**Problem**: Fine-tuning 8B parameters requires:
- 32GB+ VRAM (FP16 weights + gradients + optimizer states)
- Hours of training time
- Expensive hardware

**Solution**: LoRA adds small trainable matrices

**How LoRA Works**:
```
Original Model:        LoRA Adaptation:
W (8B params)    â†’    W (frozen) + Î”W (42M params)

Î”W = B @ A             where B: (d Ã— r), A: (r Ã— d)
                       r = rank (e.g., 16)

Total trainable: r Ã— d Ã— 2 = 16 Ã— 4096 Ã— 2 = ~130M per layer
Across all layers: 41,943,040 parameters (0.52% of 8B)
```

**Benefits**:
- Only train 0.52% of parameters
- Fits in 12GB VRAM (vs 32GB+ for full fine-tuning)
- 10x faster training
- Same quality as full fine-tuning (proven in research)

**Configuration**:
```python
model = FastLanguageModel.get_peft_model(
    model,
    r=16,              # Rank (higher = more capacity, more memory)
    lora_alpha=16,     # Scaling factor (typically = r)
    lora_dropout=0,    # No dropout for small datasets
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention layers
        "gate_proj", "up_proj", "down_proj"       # Feed-forward layers
    ],
)
```

### 4-Bit Quantization (QLoRA)

**Problem**: Even with LoRA, base model still uses 16GB (FP16)

**Solution**: Load base model in 4-bit format

**How Quantization Works**:
```
FP16 (16-bit):  2 bytes per parameter
    8B params Ã— 2 bytes = 16GB

NF4 (4-bit):    0.5 bytes per parameter
    8B params Ã— 0.5 bytes = 4GB

Compression: 4x smaller
Quality loss: <2% (NormalFloat optimized for neural networks)
```

**Implementation**:
```python
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-instruct-bnb-4bit",  # Pre-quantized
    load_in_4bit=True,  # Load in 4-bit
    dtype=None,         # Auto-detect (FP16 or BF16)
)
```

**Result**:
- Base model: 4GB (instead of 16GB)
- LoRA weights: ~200MB
- Gradients + optimizer: ~6GB
- **Total**: ~12GB (fits in Colab T4)

### Training Process

**Dataset Format** (ShareGPT style):
```json
{
  "conversations": [
    {"from": "human", "value": "User question or statement"},
    {"from": "gpt", "value": "AI Socratic response"}
  ],
  "metadata": {"topic": "React Virtual DOM"}
}
```

**Training Arguments**:
```python
TrainingArguments(
    per_device_train_batch_size=2,      # Small batch fits in memory
    gradient_accumulation_steps=4,       # Effective batch = 2Ã—4 = 8
    num_train_epochs=3,                  # 3 passes through data
    learning_rate=2e-4,                  # Standard for LoRA
    fp16=True,                           # Mixed precision (faster)
    logging_steps=1,                     # Log every step
    optim="adamw_8bit",                  # 8-bit optimizer (saves memory)
    weight_decay=0.01,                   # L2 regularization
    warmup_steps=5,                      # Gradual learning rate increase
    lr_scheduler_type="linear",          # Linear decay
)
```

**Training Metrics**:
```
Dataset: 40 examples
Batch size: 8 (2 Ã— 4 accumulation)
Steps per epoch: 40 / 8 = 5
Total steps: 5 Ã— 3 epochs = 15 steps

Time per step: ~60 seconds
Total training: ~15 minutes

Initial loss: ~2.0
Final loss: ~0.5 (75% reduction)
```

### GGUF Export

**Why GGUF?**
- **G**PT-**G**enerated **U**nified **F**ormat
- Optimized for CPU inference (llama.cpp)
- Supports various quantization levels
- Compatible with Ollama, LM Studio, etc.
- Efficient memory mapping

**Export Process**:
```python
# Merge LoRA weights with base model
model.save_pretrained_merged("merged_model", tokenizer)

# Convert to GGUF with quantization
model.save_pretrained_gguf(
    "model",
    tokenizer,
    quantization_method="q4_k_m"  # 4-bit, medium quality
)

# Output: llama-3-8b-instruct.Q4_K_M.gguf (4.9GB)
```

**Quantization Options**:
- `q4_0`: Smallest (3.5GB), lowest quality
- `q4_k_m`: Balanced (4.9GB), good quality â† **We use this**
- `q5_k_m`: Higher quality (5.5GB)
- `q8_0`: Highest quality (7.5GB)

### Ollama Deployment

**Installation**:
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh
```

**Model Import**:
```bash
# Create Modelfile (like Dockerfile)
cat > Modelfile <<EOF
FROM ./llama-3-8b-instruct.Q4_K_M.gguf
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
SYSTEM "You are a Socratic technical interviewer. Ask guiding questions."
EOF

# Import model
ollama create socratic-interviewer -f Modelfile

# Verify
ollama list
# NAME                           SIZE    MODIFIED
# socratic-interviewer:latest    4.9 GB  5 seconds ago
```

**Usage**:
```bash
# CLI
ollama run socratic-interviewer

# API (used by Next.js)
curl http://localhost:11434/api/chat -d '{
  "model": "socratic-interviewer",
  "messages": [{"role": "user", "content": "Hello"}],
  "stream": true
}'
```

---

## 10. Styling & UI/UX

### Tailwind CSS Configuration

**Installation**:
```bash
npm install -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

**Configuration** (`tailwind.config.ts`):
```typescript
import type { Config } from "tailwindcss";

const config: Config = {
  content: [
    "./pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        // Shadcn UI variables
        background: "hsl(var(--background))",
        foreground: "hsl(var(--foreground))",
        // ... other custom colors
      },
    },
  },
  plugins: [],
};

export default config;
```

**Global Styles** (`app/globals.css`):
```css
@tailwind base;
@tailwind components;
@tailwind utilities;

@layer base {
  :root {
    --background: 0 0% 100%;
    --foreground: 222.2 84% 4.9%;
    /* ... other CSS variables */
  }
}
```

### Design System

**Color Palette**:
- **Primary**: Blue-600 (interviews, actions)
- **Background**: Gray-50 (light, clean)
- **Cards**: White (content containers)
- **Text**: Gray-900 (primary), Gray-600 (secondary)
- **Success**: Green-500
- **Error**: Red-500

**Typography**:
- **Headings**: Font-bold, tracking-tight
- **Body**: Font-normal, leading-relaxed
- **Sizes**: text-sm (12px), text-base (16px), text-lg (18px), text-xl+

**Spacing**:
- **Tight**: space-y-2 (0.5rem)
- **Normal**: space-y-4 (1rem)
- **Loose**: space-y-8 (2rem)

### Component Styling Patterns

**Card Pattern**:
```tsx
<Card className="w-full max-w-2xl p-8">
  {/* Content */}
</Card>
```

**Button Variants**:
```tsx
<Button variant="default">Send</Button>      // Blue, primary action
<Button variant="outline">Cancel</Button>    // White with border
<Button variant="ghost">Reset</Button>       // Transparent
```

**Message Bubbles**:
```tsx
// User message (right-aligned, blue)
<Card className="max-w-[80%] p-4 bg-blue-600 text-white">
  {content}
</Card>

// Assistant message (left-aligned, white)
<Card className="max-w-[80%] p-4 bg-white text-gray-900">
  {content}
</Card>
```

**Responsive Design**:
```tsx
// Mobile-first approach
<div className="p-4 md:p-6 lg:p-8">
  // 4 padding on mobile, 6 on tablet, 8 on desktop
</div>

<div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3">
  // 1 column mobile, 2 tablet, 3 desktop
</div>
```

### UI/UX Principles

**1. Progressive Disclosure**:
- Show upload screen first
- Only show chat after resume uploaded
- Don't overwhelm with options

**2. Visual Feedback**:
- Loading spinners during processing
- Disabled states on buttons
- Error messages in red cards
- Success states (implicit)

**3. Accessibility**:
- Semantic HTML (`<header>`, `<form>`, `<button>`)
- ARIA labels where needed
- Keyboard navigation support
- Focus states visible

**4. Performance**:
- Lazy loading (Next.js automatic)
- Optimized images (Next.js Image component)
- Code splitting (App Router automatic)
- Streaming responses (progressive rendering)

---

## 11. Type System & Type Safety

### Type Definitions

**Location**: `types/resume.ts`

```typescript
export interface ResumeData {
  name: string;
  email?: string;
  phone?: string;
  skills: string[];
  experience: Experience[];
  education: Education[];
  weaknesses?: string[];
  rawText?: string;
}

export interface Experience {
  position: string;
  company: string;
  duration: string;
  description: string;
}

export interface Education {
  degree: string;
  institution: string;
  year: string;
}

export interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
}
```

**Usage Across Codebase**:
```typescript
// Server Action
export async function extractResumeData(
  formData: FormData
): Promise<{
  success: boolean;
  data?: ResumeData;  // Type-safe return
  error?: string;
}> { ... }

// Component Props
interface ChatInterfaceProps {
  resumeData: ResumeData;  // Type-safe props
  onReset: () => void;
}

// API Request
const { messages, resumeData } = body as {
  messages: { role: string; content: string }[];
  resumeData: ResumeData;  // Type-safe API
};
```

### TypeScript Configuration

**`tsconfig.json`**:
```json
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,              // Strict mode enabled
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [{ "name": "next" }],
    "paths": {
      "@/*": ["./*"]            // Path alias for imports
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx"],
  "exclude": ["node_modules"]
}
```

**Strict Mode Benefits**:
- `strictNullChecks`: Prevents null/undefined errors
- `strictFunctionTypes`: Ensures type-safe callbacks
- `noImplicitAny`: Requires explicit types
- `strictBindCallApply`: Type-safe function methods

### Type Safety Examples

**Before (JavaScript)**:
```javascript
// No type safety
const data = await extractResumeData(formData);
console.log(data.namee);  // Typo! No error until runtime
```

**After (TypeScript)**:
```typescript
// Type safety catches errors
const result = await extractResumeData(formData);
if (result.success && result.data) {
  console.log(result.data.namee);  // âŒ Compile error: Property 'namee' does not exist
  console.log(result.data.name);   // âœ… Correct
}
```

---

## 12. Error Handling & Resilience

### Frontend Error Handling

**ResumeUpload Component**:
```typescript
try {
  const result = await extractResumeData(formData);
  if (result.success && result.data) {
    onResumeExtracted(result.data);
  } else {
    setError(result.error || 'Failed to process resume');
  }
} catch (err: any) {
  setError(err.message || 'Failed to upload resume');
}
```

**ChatInterface Component**:
```typescript
try {
  const response = await fetch('/api/chat', { ... });
  if (!response.ok) {
    throw new Error('Failed to get response');
  }
  // Process streaming response
} catch (error) {
  console.error('Chat error:', error);
  setMessages(prev => [...prev, {
    id: Date.now().toString(),
    role: 'assistant',
    content: 'Sorry, I encountered an error. Please try again.',
  }]);
}
```

### Backend Error Handling

**Hybrid Fallback in API Route**:
```typescript
try {
  // Try Groq (Primary)
  return streamText({ model: groq(...) });
} catch (groqError) {
  // Check if rate limit
  const isRateLimit = groqError?.status === 429;

  if (isRateLimit) {
    console.log('âš ï¸ Rate limit, falling back');
  }

  try {
    // Try Gemini (Fallback)
    return streamText({ model: google(...) });
  } catch (geminiError) {
    // Both failed
    throw geminiError;
  }
}
```

**Resume Processing Error Handling**:
```typescript
try {
  const rawText = await new Promise((resolve, reject) => {
    new PdfReader().parseBuffer(buffer, (err, item) => {
      if (err) reject(err);
      // ... parsing logic
    });
  });

  if (!rawText || rawText.length < 100) {
    return {
      success: false,
      error: 'Unable to extract sufficient text from PDF'
    };
  }

  // Continue processing...
} catch (error: any) {
  console.error('âŒ Resume extraction error:', error);
  return {
    success: false,
    error: error.message || 'Failed to process resume'
  };
}
```

### Error Categories

**1. User Errors** (400-level):
- Invalid file type
- File too large
- Missing required data
- **Handling**: Show user-friendly message, allow retry

**2. Server Errors** (500-level):
- PDF parsing failure
- AI API unavailable
- Database errors (if added)
- **Handling**: Log error, show generic message, implement retry logic

**3. Network Errors**:
- Offline/no internet
- Timeout
- Connection reset
- **Handling**: Detect offline state, queue requests, retry on reconnect

**4. Rate Limit Errors** (429):
- Groq free tier exceeded
- **Handling**: Automatic fallback to Gemini

### Resilience Patterns

**1. Retry Logic** (Future Enhancement):
```typescript
async function fetchWithRetry(
  url: string,
  options: RequestInit,
  maxRetries = 3
) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await fetch(url, options);
    } catch (error) {
      if (i === maxRetries - 1) throw error;
      await new Promise(r => setTimeout(r, 1000 * (i + 1))); // Exponential backoff
    }
  }
}
```

**2. Timeout Protection**:
```typescript
export const maxDuration = 30;  // Next.js API route timeout (30s)

// Frontend timeout
const timeoutId = setTimeout(() => {
  controller.abort();  // Abort fetch if takes too long
}, 25000);  // 25 seconds
```

**3. Graceful Degradation**:
- If Groq fails â†’ Use Gemini
- If both fail â†’ Show error message
- If PDF parsing fails â†’ Ask user to try different file
- If streaming breaks â†’ Show partial response + error

---

## Summary

### Technology Choices Rationale

**Next.js 15 (App Router)**:
- Server Components by default (better performance)
- Server Actions (simpler than API routes for file uploads)
- Built-in streaming support
- Automatic code splitting
- Vercel deployment optimized

**TypeScript**:
- Type safety prevents runtime errors
- Better IDE autocomplete
- Self-documenting code
- Required for production-grade apps

**Tailwind CSS**:
- Utility-first (fast development)
- No CSS files to manage
- Consistent design tokens
- Excellent responsive utilities

**Vercel AI SDK**:
- Provider-agnostic (Groq, Gemini, OpenAI, etc.)
- Streaming built-in
- Type-safe
- Well-documented

**pdfreader**:
- Pure Node.js (no browser deps)
- Event-based (flexible)
- No complex setup
- Actually works! (after 3 library failures)

### Architectural Highlights

**1. Hybrid AI Strategy**:
- Cloud (fast, powerful) + Local (private, custom)
- Automatic fallback (resilient)
- Zero cost (free tiers + local)

**2. Two-Phase Resume Processing**:
- Phase 1: Deterministic text extraction
- Phase 2: AI-powered structuring
- Separation of concerns, easier debugging

**3. Streaming Everything**:
- AI responses stream in real-time
- Better UX (see response as it generates)
- Lower perceived latency

**4. Type Safety Throughout**:
- Frontend types match backend types
- Compile-time error detection
- Self-documenting interfaces

**5. Component-Based Architecture**:
- Reusable UI components (Shadcn)
- Clear separation of concerns
- Easy to test and maintain

---

## Files Reference

**Frontend**:
- `app/page.tsx` - Main entry point
- `app/layout.tsx` - Root layout
- `app/globals.css` - Global styles
- `components/ResumeUpload.tsx` - Upload component
- `components/ChatInterface.tsx` - Chat UI
- `components/ui/*` - Shadcn components

**Backend**:
- `app/api/chat/route.ts` - Chat API (cloud)
- `lib/actions/resume.ts` - Resume processing
- `lib/prompts.ts` - System prompt generation

**Types**:
- `types/resume.ts` - TypeScript interfaces

**ML**:
- `scripts/generate_dataset.py` - Synthetic data generation
- `notebooks/train_socratic_interviewer.ipynb` - Training pipeline
- `data/interviewer_training_data.jsonl` - Training data (40 examples)

**Config**:
- `package.json` - Dependencies
- `tsconfig.json` - TypeScript config
- `tailwind.config.ts` - Tailwind config
- `.env.local` - API keys (not in repo)

---

**Total Lines of Code**: ~2,500 lines (excluding node_modules)
**Documentation**: 10,500+ words (PROJECT_COMPLETE_EXPLANATION.md)
**Training Data**: 40 examples, 23KB
**Model Size**: 4.9GB (GGUF Q4_K_M)

This is a production-ready, thesis-grade AI/ML engineering project demonstrating both modern web development and advanced ML techniques.
