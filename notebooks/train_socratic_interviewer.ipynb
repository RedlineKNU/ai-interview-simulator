{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Llama-3-8B for Socratic Technical Interviews\n",
        "\n",
        "## Bachelor's Thesis: Custom SLM Training\n",
        "\n",
        "**Goal**: Train a Small Language Model (8B parameters) to act as a Socratic technical interviewer.\n",
        "\n",
        "**Why This Matters for ML Engineering**:\n",
        "- Demonstrates understanding of model fine-tuning\n",
        "- Shows ability to work with resource constraints (free GPU)\n",
        "- Implements modern techniques (LoRA, 4-bit quantization)\n",
        "- Creates a specialized model for a specific task\n",
        "\n",
        "**Hardware**: Google Colab Free Tier (T4 GPU, 15GB VRAM)\n",
        "\n",
        "**Techniques Used**:\n",
        "1. **4-bit Quantization**: Reduces model size from 32GB to ~4GB (8x compression)\n",
        "2. **LoRA (Low-Rank Adaptation)**: Only trains 0.1% of parameters, 10x faster\n",
        "3. **Unsloth**: 2x faster training, 70% less memory vs vanilla HuggingFace\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Environment Setup\n",
        "\n",
        "### Install Unsloth\n",
        "\n",
        "Unsloth is an optimized training library that makes fine-tuning on free GPUs possible.\n",
        "\n",
        "**Why Unsloth?**\n",
        "- Manual PyTorch: Would need 40GB+ VRAM for 8B model\n",
        "- Standard HuggingFace: Would need 20GB+ VRAM\n",
        "- Unsloth: Works in 15GB VRAM (Colab free tier!)\n",
        "\n",
        "**How it works**:\n",
        "- Custom CUDA kernels for faster attention\n",
        "- Smart gradient checkpointing\n",
        "- Optimized memory layout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also install xformers for memory-efficient attention\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Load Pre-trained Model with 4-bit Quantization\n",
        "\n",
        "### Understanding Quantization\n",
        "\n",
        "**Normal Model (FP32/FP16)**:\n",
        "- Each parameter = 32 bits (or 16 bits)\n",
        "- 8 billion parameters √ó 16 bits = 16GB memory\n",
        "\n",
        "**4-bit Quantized Model (NF4)**:\n",
        "- Each parameter = 4 bits\n",
        "- 8 billion parameters √ó 4 bits = 4GB memory\n",
        "- **Result**: 4x memory reduction!\n",
        "\n",
        "**Quality Trade-off**:\n",
        "- Minimal accuracy loss (<2% on benchmarks)\n",
        "- Perfect for fine-tuning scenarios\n",
        "- Uses NF4 (NormalFloat4) - specially designed for neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 2048  # Supports up to 2048 token context\n",
        "dtype = None  # Auto-detect: Float16 for Tesla T4, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # CRITICAL: Enables 4-bit quantization\n",
        "\n",
        "# Load model and tokenizer\n",
        "# This downloads a pre-quantized version (saves time!)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    # token = \"hf_...\", # Use if you want to use gated models (optional)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"üìä Model size: ~4GB (quantized from 16GB)\")\n",
        "print(f\"üéØ Context length: {max_seq_length} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Configure LoRA (Low-Rank Adaptation)\n",
        "\n",
        "### Why LoRA?\n",
        "\n",
        "**Problem**: Fine-tuning all 8 billion parameters requires:\n",
        "- 100GB+ VRAM\n",
        "- Days of training\n",
        "- Massive datasets\n",
        "\n",
        "**Solution**: LoRA (Hu et al., 2021)\n",
        "\n",
        "**Key Insight**: \n",
        "Most model adaptations happen in a low-dimensional subspace. Instead of updating the full weight matrix W, inject trainable low-rank decomposition:\n",
        "\n",
        "```\n",
        "W_new = W_frozen + B √ó A\n",
        "```\n",
        "\n",
        "Where:\n",
        "- W_frozen: Original 8B parameters (unchanged)\n",
        "- B √ó A: Low-rank matrices (only 0.1% parameters)\n",
        "\n",
        "**Parameters**:\n",
        "- `r` (rank): Dimension of low-rank matrices. Higher = more capacity, more memory\n",
        "  - r=8: Ultra-lightweight (recommended for <1000 examples)\n",
        "  - r=16: Balanced (good for 1000-10000 examples)\n",
        "  - r=32: High-capacity (for 10000+ examples)\n",
        "\n",
        "- `lora_alpha`: Scaling factor. Higher = stronger adaptation\n",
        "  - Typically set to r or 2√ór\n",
        "  - We use 16 to match our rank\n",
        "\n",
        "- `target_modules`: Which layers to apply LoRA\n",
        "  - q_proj, k_proj, v_proj: Attention mechanisms (most important)\n",
        "  - o_proj: Output projection\n",
        "  - gate_proj, up_proj, down_proj: MLP layers\n",
        "\n",
        "**Memory Savings**:\n",
        "- Full fine-tuning: 8B parameters to train\n",
        "- LoRA (r=16): ~8M parameters to train (1000x reduction!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Rank: 16 is good balance for our ~50 examples\n",
        "    target_modules=[\n",
        "        \"q_proj\",   # Query projection (attention)\n",
        "        \"k_proj\",   # Key projection (attention)\n",
        "        \"v_proj\",   # Value projection (attention)\n",
        "        \"o_proj\",   # Output projection (attention)\n",
        "        \"gate_proj\", # MLP gate\n",
        "        \"up_proj\",   # MLP up\n",
        "        \"down_proj\", # MLP down\n",
        "    ],\n",
        "    lora_alpha=16,  # Scaling factor (typically = r)\n",
        "    lora_dropout=0,  # No dropout (adds randomness, we have small dataset)\n",
        "    bias=\"none\",     # Don't train bias terms (saves memory)\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Smart checkpointing (saves VRAM)\n",
        "    random_state=3407,  # Reproducibility\n",
        "    use_rslora=False,   # Rank-stabilized LoRA (not needed for our case)\n",
        "    loftq_config=None,  # LoftQ quantization (not needed)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA adapters configured!\")\n",
        "print(f\"üìä Trainable parameters: ~{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.1f}M\")\n",
        "print(f\"üîí Frozen parameters: ~8000M\")\n",
        "print(f\"‚ö° Memory efficient: Only training {(sum(p.numel() for p in model.parameters() if p.requires_grad) / 8e9 * 100):.3f}% of model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Load Training Dataset\n",
        "\n",
        "### Dataset Format: ShareGPT\n",
        "\n",
        "Our JSONL file contains conversations in this format:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"conversations\": [\n",
        "    {\"from\": \"human\", \"value\": \"React uses real DOM directly.\"},\n",
        "    {\"from\": \"gpt\", \"value\": \"Not quite. React maintains an in-memory representation to optimize updates. Do you remember what that concept is called?\"}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "**Why ShareGPT format?**\n",
        "- Standard format for chat models\n",
        "- Supports multi-turn conversations\n",
        "- Compatible with Unsloth/TRL trainers\n",
        "\n",
        "**Upload Instructions**:\n",
        "1. In Colab, click the folder icon (left sidebar)\n",
        "2. Upload `interviewer_training_data.jsonl`\n",
        "3. Or use Google Drive mount (code below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "# 1) Load your existing synthetic interviewer data\n",
        "custom = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"interviewer_training_data.jsonl\",\n",
        "    split=\"train\",\n",
        ")\n",
        "\n",
        "# 2) Load the Socratic dataset from Hugging Face\n",
        "#    (full split is ~50k rows; we sample for Colab to avoid OOM)\n",
        "socratic_full = load_dataset(\"facat/Socratic\", split=\"train\")\n",
        "\n",
        "# Optional: subsample Socratic to keep training light (e.g. 5k examples)\n",
        "socratic = socratic_full.shuffle(seed=3407).select(range(5000))\n",
        "\n",
        "# 3) Keep only the `conversations` field in both, to match later formatting\n",
        "\n",
        "def keep_conversations(ds):\n",
        "    cols_to_drop = [c for c in ds.column_names if c != \"conversations\"]\n",
        "    return ds.remove_columns(cols_to_drop) if cols_to_drop else ds\n",
        "\n",
        "custom = keep_conversations(custom)\n",
        "socratic = keep_conversations(socratic)\n",
        "\n",
        "# 4) Merge them\n",
        "dataset = concatenate_datasets([custom, socratic])\n",
        "\n",
        "print(f\"‚úÖ Merged dataset loaded: {len(dataset)} examples\")\n",
        "print(\"   - Custom examples:\", len(custom))\n",
        "print(\"   - Socratic examples:\", len(socratic))\n",
        "print(\"üìù Sample conversation:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Format Dataset for Training\n",
        "\n",
        "### Chat Template\n",
        "\n",
        "We need to format conversations into the model's expected format:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a Socratic technical interviewer...<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "React uses real DOM directly.<|eot_id|>\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "Not quite. React maintains...<|eot_id|>\n",
        "```\n",
        "\n",
        "Unsloth handles this automatically with the `standardize_sharegpt` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Apply Llama-3 chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3\",  # Use Llama-3's official format\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
        ")\n",
        "\n",
        "# Function to format conversations\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    Format conversations into model's expected structure.\n",
        "\n",
        "    Adds system prompt to guide Socratic behavior.\n",
        "    \"\"\"\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = []\n",
        "\n",
        "    for convo in convos:\n",
        "        # Add system message at the start\n",
        "        full_convo = [\n",
        "            {\n",
        "                \"from\": \"system\",\n",
        "                \"value\": \"You are a Senior Technical Interviewer who uses the Socratic method. \" + \n",
        "                         \"Ask guiding questions instead of providing direct answers. \" +\n",
        "                         \"Help candidates discover solutions through questioning.\"\n",
        "            }\n",
        "        ] + convo\n",
        "\n",
        "        # Apply chat template\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            full_convo,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply formatting to entire dataset\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Dataset formatted for training\")\n",
        "print(f\"üìù Sample formatted text (first 500 chars):\")\n",
        "print(dataset[0][\"text\"][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Training Configuration\n",
        "\n",
        "### SFTTrainer (Supervised Fine-Tuning)\n",
        "\n",
        "**Hyperparameters Explained**:\n",
        "\n",
        "**Learning Rate (2e-4)**:\n",
        "- How fast the model adapts\n",
        "- Too high: Model forgets original knowledge (catastrophic forgetting)\n",
        "- Too low: Training takes forever\n",
        "- 2e-4 is sweet spot for LoRA fine-tuning\n",
        "\n",
        "**Batch Size (2)**:\n",
        "- How many examples processed at once\n",
        "- Limited by GPU memory (T4 has 15GB)\n",
        "- We use gradient accumulation to simulate larger batches\n",
        "\n",
        "**Gradient Accumulation (4)**:\n",
        "- Simulates batch size of 2√ó4 = 8\n",
        "- Updates weights every 4 steps\n",
        "- Saves memory while maintaining training quality\n",
        "\n",
        "**Epochs (3)**:\n",
        "- How many times to see each example\n",
        "- Too few: Underfitting\n",
        "- Too many: Overfitting (memorization)\n",
        "- 3 epochs is good for ~50 examples\n",
        "\n",
        "**Warmup Steps (5)**:\n",
        "- Gradually increase learning rate at start\n",
        "- Prevents initial instability\n",
        "- 10% of total steps is common\n",
        "\n",
        "**Weight Decay (0.01)**:\n",
        "- L2 regularization to prevent overfitting\n",
        "- Penalizes large weights\n",
        "- 0.01 is standard for small datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",  # Field containing formatted conversations\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,  # Parallel data loading\n",
        "    packing=False,  # Don't pack multiple examples (better for chat)\n",
        "    args=TrainingArguments(\n",
        "        # Output\n",
        "        output_dir=\"outputs\",\n",
        "        \n",
        "        # Training schedule\n",
        "        per_device_train_batch_size=2,  # Batch size per GPU\n",
        "        gradient_accumulation_steps=4,   # Simulate batch size of 8\n",
        "        num_train_epochs=3,              # 3 passes through dataset\n",
        "        \n",
        "        # Optimization\n",
        "        learning_rate=2e-4,              # LoRA sweet spot\n",
        "        fp16=not torch.cuda.is_bf16_supported(),  # Use FP16 if no BF16\n",
        "        bf16=torch.cuda.is_bf16_supported(),       # Use BF16 if available\n",
        "        \n",
        "        # Regularization\n",
        "        warmup_steps=5,                  # Learning rate warmup\n",
        "        weight_decay=0.01,               # L2 regularization\n",
        "        \n",
        "        # Logging\n",
        "        logging_steps=1,                 # Log every step (we have few steps)\n",
        "        optim=\"adamw_8bit\",              # 8-bit AdamW (saves memory)\n",
        "        \n",
        "        # Saving\n",
        "        save_strategy=\"epoch\",           # Save after each epoch\n",
        "        save_total_limit=2,              # Keep only last 2 checkpoints\n",
        "        \n",
        "        # Performance\n",
        "        seed=3407,                       # Reproducibility\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configured\")\n",
        "print(f\"üìä Effective batch size: {2 * 4} (2 √ó 4 gradient accumulation)\")\n",
        "print(f\"üî¢ Total training steps: ~{len(dataset) * 3 // (2 * 4)}\")\n",
        "print(f\"‚è±Ô∏è Estimated training time: ~15-30 minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Train the Model! üöÄ\n",
        "\n",
        "This will take 15-30 minutes on Colab Free (T4 GPU).\n",
        "\n",
        "**What to Watch**:\n",
        "- Loss should decrease (from ~2.0 to ~0.5)\n",
        "- If loss stops decreasing: might be overfitting\n",
        "- If loss is erratic: learning rate might be too high\n",
        "\n",
        "**Memory Usage**:\n",
        "- Peak VRAM: ~12-14GB (within T4's 15GB)\n",
        "- If OOM (Out of Memory): Reduce batch size to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show GPU memory before training\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"üéÆ GPU: {gpu_stats.name}\")\n",
        "print(f\"üíæ Memory: {start_gpu_memory} GB / {max_memory} GB\")\n",
        "print()\n",
        "print(\"üöÄ Starting training...\")\n",
        "print()\n",
        "\n",
        "# Train!\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Show final stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "\n",
        "print()\n",
        "print(\"‚úÖ Training complete!\")\n",
        "print(f\"‚è±Ô∏è Time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"üìâ Final loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
        "print(f\"üíæ Peak memory: {used_memory} GB ({used_percentage}% of {max_memory} GB)\")\n",
        "print(f\"üéØ Memory for LoRA: {used_memory_for_lora} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Test the Model\n",
        "\n",
        "Let's see if our Socratic interviewer works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test conversation\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a Senior Technical Interviewer who uses the Socratic method. \" +\n",
        "                   \"Ask guiding questions instead of providing direct answers.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"React uses the real DOM directly, right?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Format with chat template\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "# Decode and print\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(\"üßë‚Äçüíº Interviewer Response:\")\n",
        "print(response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].split(\"<|eot_id|>\")[0].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Save the Model (HuggingFace Format)\n",
        "\n",
        "Save LoRA adapters for future use or sharing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapters\n",
        "model.save_pretrained(\"socratic_interviewer_lora\")\n",
        "tokenizer.save_pretrained(\"socratic_interviewer_lora\")\n",
        "\n",
        "print(\"‚úÖ Model saved to 'socratic_interviewer_lora/'\")\n",
        "print(\"üì¶ Files:\")\n",
        "!ls -lh socratic_interviewer_lora/\n",
        "\n",
        "# Optional: Push to HuggingFace Hub\n",
        "# model.push_to_hub(\"your-username/socratic-interviewer\", token=\"hf_...\")\n",
        "# tokenizer.push_to_hub(\"your-username/socratic-interviewer\", token=\"hf_...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Export to GGUF (For Local Inference with Ollama)\n",
        "\n",
        "### What is GGUF?\n",
        "\n",
        "**GGUF (GPT-Generated Unified Format)** is a file format for storing LLMs, designed by the llama.cpp team.\n",
        "\n",
        "**Why GGUF?**:\n",
        "- Runs on CPU (no GPU needed!)\n",
        "- Quantized (4-bit, 5-bit, 8-bit options)\n",
        "- Fast inference with llama.cpp\n",
        "- Compatible with Ollama (easy local deployment)\n",
        "\n",
        "**Quantization Levels**:\n",
        "- Q4_K_M: 4-bit, balanced (recommended) - ~4.5GB\n",
        "- Q5_K_M: 5-bit, higher quality - ~5.5GB\n",
        "- Q8_0: 8-bit, maximum quality - ~8GB\n",
        "\n",
        "For thesis: **Q4_K_M** is perfect balance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA adapters with base model (required for GGUF export)\n",
        "model.save_pretrained_merged(\n",
        "    \"socratic_interviewer_merged\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",  # Save as FP16 (smaller than FP32)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model merged (LoRA + base model)\")\n",
        "\n",
        "# Export to GGUF format\n",
        "model.save_pretrained_gguf(\n",
        "    \"socratic_interviewer_gguf\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",  # 4-bit quantization (balanced)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GGUF model exported!\")\n",
        "print(\"üì¶ Files:\")\n",
        "!ls -lh socratic_interviewer_gguf/\n",
        "\n",
        "print()\n",
        "print(\"üì• Download the .gguf file to your local machine\")\n",
        "print(\"   (Click folder icon ‚Üí right-click .gguf file ‚Üí Download)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 11: Alternative Export Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export different quantization levels (optional)\n",
        "\n",
        "# Q5_K_M: Higher quality, slightly larger\n",
        "# model.save_pretrained_gguf(\n",
        "#     \"socratic_interviewer_q5\",\n",
        "#     tokenizer,\n",
        "#     quantization_method=\"q5_k_m\",\n",
        "# )\n",
        "\n",
        "# Q8_0: Maximum quality, largest size\n",
        "# model.save_pretrained_gguf(\n",
        "#     \"socratic_interviewer_q8\",\n",
        "#     tokenizer,\n",
        "#     quantization_method=\"q8_0\",\n",
        "# )\n",
        "\n",
        "print(\"‚ÑπÔ∏è For thesis, Q4_K_M is recommended (best balance)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéâ Training Complete!\n",
        "\n",
        "### What You've Accomplished:\n",
        "\n",
        "‚úÖ Fine-tuned an 8B parameter model on free hardware\n",
        "‚úÖ Used modern techniques: LoRA, 4-bit quantization, Unsloth\n",
        "‚úÖ Created a specialized Socratic interviewer\n",
        "‚úÖ Exported model for local inference (GGUF)\n",
        "\n",
        "### For Your Thesis:\n",
        "\n",
        "**Technical Contributions**:\n",
        "1. Demonstrated resource-constrained ML Engineering\n",
        "2. Implemented parameter-efficient fine-tuning (PEFT)\n",
        "3. Created specialized dataset for Socratic teaching\n",
        "4. Deployed custom model locally (cost-effective)\n",
        "\n",
        "**Key Metrics to Report**:\n",
        "- Base model: Llama-3-8B (8 billion parameters)\n",
        "- Trainable parameters: ~8M (0.1% of total)\n",
        "- Training time: ~20 minutes on T4 GPU\n",
        "- Memory usage: ~12GB VRAM\n",
        "- Final model size: ~4.5GB (GGUF Q4_K_M)\n",
        "- Quantization: 4-bit (4x compression vs FP16)\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. Download the `.gguf` file\n",
        "2. Install Ollama locally\n",
        "3. Import model to Ollama\n",
        "4. Integrate into Next.js app\n",
        "5. Compare with Groq/Gemini performance\n",
        "\n",
        "---\n",
        "\n",
        "**References for Thesis**:\n",
        "- LoRA: Hu et al. (2021) - \"LoRA: Low-Rank Adaptation of Large Language Models\"\n",
        "- Quantization: Dettmers et al. (2023) - \"QLoRA: Efficient Finetuning of Quantized LLMs\"\n",
        "- Llama-3: Meta (2024) - \"The Llama 3 Herd of Models\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
